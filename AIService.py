# -*- coding: utf-8 -*-
"""AIMotor5M

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zDjasrfKcygZP_BndjluoiCgCPzb99lA
"""

!pip install -q -U torch datasets transformers==4.36.1 tensorflow langchain playwright html2text sentence_transformers faiss-cpu

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline

from datasets import load_dataset
from peft import LoraConfig, PeftModel

from langchain.text_splitter import CharacterTextSplitter
from langchain.document_transformers import Html2TextTransformer
from langchain.document_loaders import AsyncChromiumLoader

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.llms import HuggingFacePipeline
from langchain.chains import LLMChain

from huggingface_hub import login

from google.colab import userdata
key = userdata.get('HUGGINFACE_KEY')

login(key)

model_name='mistralai/Mistral-7B-Instruct-v0.2'

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

#################################################################
# bitsandbytes parameters
#################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

#################################################################
# Set up quantization config
#################################################################
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)
#################################################################
# bitsandbytes parameters
#################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

#################################################################
# Set up quantization config
#################################################################
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

#################################################################
# Load pre-trained config
#################################################################
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
)

def print_number_of_trainable_model_parameters(model):
    trainable_model_params = 0
    all_model_params = 0
    for _, param in model.named_parameters():
        all_model_params += param.numel()
        if param.requires_grad:
            trainable_model_params += param.numel()
    return f"trainable model parameters: {trainable_model_params}\nall model parameters: {all_model_params}\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%"

print(print_number_of_trainable_model_parameters(model))

# Build Mistral text generation pipeline
text_generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    temperature=0.2,
    repetition_penalty=1.1,
    return_full_text=True,
    max_new_tokens=1000,
)

mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)

!playwright install
!playwright install-deps

import nest_asyncio
nest_asyncio.apply()

# Articles to index

articles = ["https://coincheckup.com/blog/best-crypto-to-buy"]

# Scrapes the blogs above
loader = AsyncChromiumLoader(articles)
docs = loader.load()

# Converts HTML to plain text
html2text = Html2TextTransformer()
docs_transformed = html2text.transform_documents(docs)

# Chunk text
text_splitter = CharacterTextSplitter(chunk_size=1000,
                                      chunk_overlap=0)
chunked_documents = text_splitter.split_documents(docs_transformed)

# Load chunked documents into the FAISS index
db = FAISS.from_documents(chunked_documents,
                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2'))

retriever = db.as_retriever()

prompt_template = """
### [INST] Instrucci√≥n: Menciona los 12 proyectos y da una breve descripcion

{context}

### PREGUNTA:
{question} (responde en castellano) [/INST]
 """

# Create prompt from prompt template
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# Create llm chain
llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)

llm_chain.invoke({"context": "", "question": "Dime las criptos mas relevantes con la informacion que se te entrego"})

from langchain_community.tools.tavily_search import TavilySearchResults
import os
os.environ["TAVILY_API_KEY"] = userdata.get('TAVILY_API_KEY')
search = TavilySearchResults()

search.invoke("what are the best crypto airdrops projects for 2024 for speculate")

"""# youtube workflow"""

import faiss
import numpy as np
import youtube_transcript_api
from langchain.agents import load_tools
from langchain.agents.agent_toolkits import create_python_toolkit
from langchain.agents import initialize_agent
from langchain.llms import HuggingFaceMistral

# Initialize FAISS index
index = faiss.IndexFlatL2(128)

# Download and transform the text from two YouTube videos
video_ids = ["video1_id", "video2_id"]
vectors = []
for video_id in video_ids:
    transcript = youtube_transcript_api.get_transcript(video_id)
    text = [segment["text"] for segment in transcript]
    vector = np.array([model.encode(sentence) for sentence in text])
    vectors.append(vector)
    index.add(vector)

# Convert the vectors to a single numpy array
vectors = np.concatenate(vectors)

# Initialize the HuggingFaceMistral model
hf_mistral = HuggingFaceMistral(model="mistral/7b")

# Define the tools
tools = [load_tools("basic_math") for _ in range(2)]

# Create the Python toolkit
python_toolkit = create_python_toolkit(tools=tools)

# Initialize the agent
agent = initialize_agent(
    agent="text-generation-agent",
    toolkit=python_toolkit,
    llm=hf_mistral,
    verbose=True
)

# Ask the agent a question and print the response
query = "What is the main idea in the two videos?"
result = agent.run(query)
print(result)

"""# option 2"""

import numpy as np
import youtube_transcript_api
from langchain.agents import load_tools
from langchain.agents.agent_toolkits import create_python_toolkit
from langchain.agents import initialize_agent
from langchain.llms import HuggingFaceMistral
from sklearn.metrics.pairwise import cosine_similarity

# Download and transform the text from two YouTube videos
video_ids = ["video1_id", "video2_id"]
vectors = []
for video_id in video_ids:
    transcript = youtube_transcript_api.get_transcript(video_id)
    text = [segment["text"] for segment in transcript]
    vector = np.array([model.encode(sentence) for sentence in text])
    vectors.append(vector)

# Convert the vectors to a single numpy array
vectors = np.concatenate(vectors)

# Initialize the HuggingFaceMistral model
hf_mistral = HuggingFaceMistral(model="mistral/7b")

# Define the tools
tools = [load_tools("basic_math") for _ in range(2)]

# Create the Python toolkit
python_toolkit = create_python_toolkit(tools=tools)

# Initialize the agent
agent = initialize_agent(
    agent="text-generation-agent",
    toolkit=python_toolkit,
    llm=hf_mistral,
    verbose=True
)

# Ask the agent a question
query = "What is the main idea in those videos?"
response = agent.run(query)

# Calculate the cosine similarity between the response and the video vectors
similarity_scores = cosine_similarity(response, vectors)

# Get the index of the most similar video
most_similar_index = np.argmax(similarity_scores)

# Get the video ID of the most similar video
most_similar_video_id = video_ids[most_similar_index]

# Get the transcript of the most similar video
transcript = youtube_transcript_api.get_transcript(most_similar_video_id)
text = [segment["text"] for segment in transcript]

# Generate a summary of the transcript
summary = agent.run("Summarize the following text: " + " ".join(text))

# Print the summary
print(f"The main idea in those videos is: {summary}")
